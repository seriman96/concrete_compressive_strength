facet_wrap(~ hp_category) +
labs(x = "Weight (wt)", y = "Miles per Gallon (mpg)",
title = "MPG vs Weight, Faceted by Horsepower Category") +
theme_minimal()
# Define the pdf function f(x)
f <- function(x) {
return(3 / (1 + x)^3)
}
c = 0.73
# Generate values of x from 0 to the value of c you found
x_vals <- seq(0, c, length.out = 100)
# Compute corresponding f(x) values
y_vals <- f(x_vals)
# Plot the pdf curve
plot(x_vals, y_vals, type = "l", col = "green", lwd = 2,
xlab = "x", ylab = "f(x)", main = "Probability Density Function of X")
# Add a grid for better readability
grid()
# Define the inverse CDF function
inverse_cdf <- function(u) {
return((1 / sqrt(1 - u)) - 1)
}
# Simulate 1000 uniform random numbers
u_vals <- runif(1000)
# Generate 1000 observations from the distribution
x_sim <- inverse_cdf(u_vals)
# Plot the histogram of the simulated data
hist(x_sim, breaks = 30, probability = TRUE, main = "Simulated Observations", xlab = "x")
# Overlay the true pdf
lines(x_vals, y_vals, col = "blue", lwd = 2)
#e. Use the generated data from part (d), estimate the mean and variance of the distribution
mean_x <- mean(x_sim)
var_x <- var(x_sim)
# Add mean and variance as text on the plot
text(x = 1.5, y = 0.8, labels = paste("Mean:", round(mean_x, 3)), col = "red", cex = 1.2)
text(x = 1.5, y = 0.7, labels = paste("Variance:", round(var_x, 3)), col = "red", cex = 1.2)
print(paste("Estimated mean is: ", mean_x))
print(paste("Estimated variance is: ", var_x))
# Add a grid for better readability
grid()
library(dplyr)
# a. Load the dataset used in the lecture and filter for movies produced after 2005
url <- "https://raw.githubusercontent.com/Juanets/movie-stats/master/movies.csv"
movies <- read.csv(url)
movies.sub <- movies %>%
filter(year > 2005)
# b. Keep only the specified columns:  name, director, year, country, genre, budget, gross, score
movies.sub <- movies.sub %>%
select(name, director, year, country, genre, budget, gross, score)
# c. Calculate profit as a fraction of budget and convert budget and gross to million dollar units
movies.sub <- movies.sub %>%
mutate(
budget = round(budget / 1e6, 1),        # Convert budget to million dollars and round
gross = round(gross / 1e6, 1),          # Convert gross to million dollars and round
profit_fraction = round((gross - budget) / budget, 2)  # Profit as a fraction of budget
)
# d. Count the number of movies by genre and order in descending order
genre_counts <- movies.sub %>%
group_by(genre) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the results
# list(
#subset_data = head(movies.sub),
#genre_counts = genre_counts
#)
# f. Group by country and genre, and then calculate the required statistics
movies_stats <- movies.sub %>%
group_by(country, genre) %>%
summarise(
count = n(),  # Count of movies in each group
median_fractional_profit = median(profit_fraction, na.rm = TRUE),  # Median of fractional profit
mean_score = mean(score, na.rm = TRUE),  # Mean of movie score
variance_score = var(score, na.rm = TRUE)  # Variance of movie score
) %>%
arrange(desc(count))  # Arrange by count in descending order
# Display the resulting dataset
# head(movies_stats)
library(dplyr)
# a. Load the dataset used in the lecture and filter for movies produced after 2005
url <- "https://raw.githubusercontent.com/Juanets/movie-stats/master/movies.csv"
movies <- read.csv(url)
movies.sub <- movies %>%
filter(year > 2005)
# b. Keep only the specified columns:  name, director, year, country, genre, budget, gross, score
movies.sub <- movies.sub %>%
select(name, director, year, country, genre, budget, gross, score)
# c. Calculate profit as a fraction of budget and convert budget and gross to million dollar units
movies.sub <- movies.sub %>%
mutate(
budget = round(budget / 1e6, 1),        # Convert budget to million dollars and round
gross = round(gross / 1e6, 1),          # Convert gross to million dollars and round
profit_fraction = round((gross - budget) / budget, 2)  # Profit as a fraction of budget
)
# d. Count the number of movies by genre and order in descending order
genre_counts <- movies.sub %>%
group_by(genre) %>%
summarise(count = n()) %>%
arrange(desc(count))
# Display the results
list(
subset_data = head(movies.sub),
genre_counts = genre_counts
)
# f. Group by country and genre, and then calculate the required statistics
movies_stats <- movies.sub %>%
group_by(country, genre) %>%
summarise(
count = n(),  # Count of movies in each group
median_fractional_profit = median(profit_fraction, na.rm = TRUE),  # Median of fractional profit
mean_score = mean(score, na.rm = TRUE),  # Mean of movie score
variance_score = var(score, na.rm = TRUE)  # Variance of movie score
) %>%
arrange(desc(count))  # Arrange by count in descending order
# Display the resulting dataset
head(movies_stats)
# Load necessary libraries
library(dplyr)
# Load the dataset used in the lecture
url <- "https://raw.githubusercontent.com/Juanets/movie-stats/master/movies.csv"
movies <- read.csv(url)
# Filter movies produced after 2001, then group, summarize, and find top directors by mean score
top_directors_by_genre <- movies %>%
filter(year > 2001) %>%                # Only movies after 2001
group_by(director) %>%                 # Group by director
filter(n() >= 4) %>%                   # Keep directors with 4 or more movies
ungroup() %>%
group_by(genre, director) %>%          # Group by genre and director
summarise(mean_score = mean(score, na.rm = TRUE), .groups = 'drop') %>%  # Calculate mean score
group_by(genre) %>%
top_n(2, mean_score) %>%               # Select the top 2 directors by mean score in each genre
arrange(genre, desc(mean_score))       # Arrange by genre and mean score in descending order
# Display the results
top_directors_by_genre
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Load the dataset
url <- "https://raw.githubusercontent.com/Juanets/movie-stats/master/EconomistData.csv"
EconomistData <- read.csv(url)
# Load necessary libraries
library(ggplot2)
library(dplyr)
# Load the dataset
EconomistData <- read.csv('./EconomistData.csv')
# Take a quick look at the data
head(EconomistData)
# a. Create a scatter plot similar to the one in the article
ggplot(EconomistData, aes(x = Percent.of.15plus.with.bank.account, y = SEDA.Current.level)) +
geom_point(color = "purple") +                     # b. Color all points blue
labs(x = "Percent of Population (15+) with Bank Account",
y = "Current SEDA Score",
title = "Relationship between Well-being and Financial Inclusion")
# c. Color points by Region
ggplot(EconomistData, aes(x = Percent.of.15plus.with.bank.account, y = SEDA.Current.level, color = Region)) +
geom_point(size = 3) +
labs(x = "Percent of Population (15+) with Bank Account",
y = "Current SEDA Score",
title = "Relationship between Well-being and Financial Inclusion by Region") +
theme_minimal()
# d. Overlay a fitted smoothing trend with a low span
ggplot(EconomistData, aes(x = Percent.of.15plus.with.bank.account, y = SEDA.Current.level, color = Region)) +
geom_point(size = 3) +
geom_smooth(span = 0.2, se = FALSE) +            # Span controls the smoothness; lower values make the line fit more closely
labs(x = "Percent of Population (15+) with Bank Account",
y = "Current SEDA Score",
title = "Relationship between Well-being and Financial Inclusion by Region (Smoothed)") +
theme_minimal()
# e. Overlay a regression line
ggplot(EconomistData, aes(x = Percent.of.15plus.with.bank.account, y = SEDA.Current.level, color = Region)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +         # Linear regression line
labs(x = "Percent of Population (15+) with Bank Account",
y = "Current SEDA Score",
title = "Relationship between Well-being and Financial Inclusion by Region (Regression)") +
theme_minimal()
# f. Facet by Region
ggplot(EconomistData, aes(x = Percent.of.15plus.with.bank.account, y = SEDA.Current.level, color = Region)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE) +         # Linear regression line
labs(x = "Percent of Population (15+) with Bank Account",
y = "Current SEDA Score",
title = "Relationship between Well-being and Financial Inclusion by Region") +
theme_minimal() +
facet_wrap(~ Region)                             # Facet the plot by Region
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- read.csv('./largest_cities.csv')
head(cities)
# movies.sub <- movies %>%
# filter(year > 2005)
library(dplyr)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, population)
selected_cities
library(dplyr)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
cities
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, population)
selected_cities
library(dplyr)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
cities
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, population)
selected_cities
# c. The numbers in (b) are very small. Modify the units in city_density by multiplying the city density by 1000.
cities <- cities %>%
mutate(city_density = city_density * 1000)
cities
library(dplyr)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
cities
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, city_density)
selected_cities
# c. The numbers in (b) are very small. Modify the units in city_density by multiplying the city density by 1000.
cities <- cities %>%
mutate(city_density = city_density * 1000)
cities
library(dplyr)
library(ggplot2)
library(ggrepel)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
cities
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, city_density)
selected_cities
# c. The numbers in (b) are very small. Modify the units in city_density by multiplying the city density by 1000.
cities <- cities %>%
mutate(city_density = city_density * 1000)
cities
# d. Now report the average city density by continent.
avg_density_by_continent <- cities %>%
group_by(continent) %>%
summarize(avg_city_density = mean(city_density, na.rm = TRUE))
avg_density_by_continent
#e. Create a plot with city density on the x-axis and metro density on the y-axis. Use a log scale for the axes and include points and text repel labels with the city names.
ggplot(cities, aes(x = city_density, y = metro_density, label = name)) +
geom_point() +
geom_text_repel() +
scale_x_log10() +
scale_y_log10() +
labs(x = "City Density (log scale)", y = "Metro Density (log scale)",
title = "City Density vs Metro Density") +
theme_minimal()
library(dplyr)
library(ggplot2)
library(ggrepel)
cities <- read.csv('./largest_cities.csv')
head(cities)
# a. create a new feature named city_density by dividing the city population city_pop by the city area city_area.
cities <- cities %>%
mutate(city_density = city_pop / city_area)
cities
# b. Use the select function to select the city name (name), population, area and density.
selected_cities <- cities %>%
select(name, city_pop, city_area, city_density)
selected_cities
# c. The numbers in (b) are very small. Modify the units in city_density by multiplying the city density by 1000.
cities <- cities %>%
mutate(city_density = city_density * 1000)
cities
# d. Now report the average city density by continent.
avg_density_by_continent <- cities %>%
group_by(continent) %>%
summarize(avg_city_density = mean(city_density, na.rm = TRUE))
avg_density_by_continent
#e. Create a plot with city density on the x-axis and metro density on the y-axis. Use a log scale for the axes and include points and text repel labels with the city names.
ggplot(cities, aes(x = city_density, y = metro_pop, label = name)) +
geom_point() +
geom_text_repel() +
scale_x_log10() +
scale_y_log10() +
labs(x = "City Density (log scale)", y = "Metro Density (log scale)",
title = "City Density vs Metro Density") +
theme_minimal()
setwd("D:/Data Science/1st_Semester/Regression_model/Proposal")
library(ggplot2)
library(car)
library(corrplot)
library(infotheo)
library(olsrr)
library(lmtest)
data <-read.csv("./Concrete Compressive Strength.csv")
head(data)
names(data) # name of present feature in the dataset
class(data$Water) # class of the given feature
str(data) # it like info() in python
summary(data) #like describe() in python
colnames(data) <- gsub("\\s+", "_", colnames(data))
correlation_matrix <- cor(data)
print(correlation_matrix)
corrplot::corrplot(correlation_matrix, method = "number") # 'arg' should be one of “circle”, “square”, “ellipse”, “number”, “shade”, “color”, “pie”
pairs(data[, c("Cement", "Blast.Furnace.Slag", "Fly.Ash",
"Water", "Superplasticizer", "Coarse.Aggregate", "Fine.Aggregate",
"Age..day.", "Concrete.compressive.strength")], main = "Scatterplot Matrix")
# Define a function for mutual information scores:
mutual_info_scores <- function(data, response_var, top_n = 3) {
# Ensure the response variable is a factor for discretization
data[[response_var]] <- as.factor(data[[response_var]])
# Convert data to a discretized version (required for mutual information)
disc_data <- discretize(data, disc = "equalfreq")
# Calculate mutual information for all predictors against the response variable
mi_scores <- sapply(colnames(disc_data), function(var) {
if (var != response_var) {
mutinformation(disc_data[[var]], disc_data[[response_var]]) # Calculates the mutual information score between two variables.
} else {
NA  # Skip the response variable itself
}
})
# Format the results into a data frame
mi_df <- data.frame(Variable = colnames(data), MI_Score = mi_scores)
mi_df <- mi_df[!is.na(mi_df$MI_Score), ]  # Remove NA values
print(mi_df)
# Plot the mutual information scores
p <- ggplot(mi_df, aes(x = reorder(Variable, MI_Score), y = MI_Score)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Mutual Information Scores", x = "Predictor Variables", y = "Mutual Information") +
theme_minimal()
print(p)
}
# Apply the function to your dataset:
# Example: Assuming the response variable is "Concrete compressive strength"
mi_results <- mutual_info_scores(data, response_var = "Concrete.compressive.strength")
## Get the top predictors
top_predictors <- c("Age..day.", "Cement", "Water")
print(top_predictors)
response_var = "Concrete.compressive.strength"
# Plot relationships for the top predictors
for (var in top_predictors) {
h <- ggplot(data, aes_string(x = var, y = response_var)) +
geom_point(alpha = 0.6, color = "blue") +
labs(title = paste("Relationship between", var, "and", response_var),
x = var, y = response_var) +
theme_minimal()
print(h)
}
# Fit the OLS model
model <- lm(Concrete.compressive.strength ~ ., data = data)
ols_regress(model) #sig. in output table represent p-value
# Define the alpha level and degrees of freedom
alpha <- 0.05
df1 <- 1    # Numerator degrees of freedom (commonly 1 for a simple test)
df2 <- 1021 # Denominator degrees of freedom
# Compute the F-value at alpha = 0.05
f_value <- qf(1 - alpha, df1, df2)
# Print the F-value
print(f_value)
# from anova table we've:
f0 <- 204.269
cat("F0 = ", f0, ">" , "F = ", f_value, "\n")
vif(model)
model1 <- lm(Concrete.compressive.strength ~ Cement, data = data)
model2 <- lm(Concrete.compressive.strength ~ Blast.Furnace.Slag, data = data)
model3 <- lm(Concrete.compressive.strength ~ Fly.Ash, data = data)
model4 <- lm(Concrete.compressive.strength ~ Water, data = data)
model5 <- lm(Concrete.compressive.strength ~ Superplasticizer, data = data)
model6 <- lm(Concrete.compressive.strength ~ Coarse.Aggregate, data = data)
model7 <- lm(Concrete.compressive.strength ~ Fine.Aggregate, data = data)
model8 <- lm(Concrete.compressive.strength ~ Age..day., data = data)
ols_regress(model1)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model2)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model3)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model4)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model5)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model6)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model7)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
ols_regress(model8)
print("Standard Errors assume that the covariance matrix of the errors is correctly specified.")
# New data point
new_data <- data.frame(
Cement = 300,
Blast.Furnace.Slag = 100,
Fly.Ash = 50,
Water = 200,
Superplasticizer = 5,
Coarse.Aggregate = 1000,
Fine.Aggregate = 800,
Age..day. = 28
)
# Predict with confidence interval
predict(model, newdata = new_data, interval = "confidence")
# Predict with prediction interval
predict(model, newdata = new_data, interval = "prediction")
# New data point
new_data <- data.frame(
Cement = 300,
Blast.Furnace.Slag = 100,
Fly.Ash = 50,
Water = 200,
Superplasticizer = 5,
Coarse.Aggregate = 1000,
Fine.Aggregate = 800,
Age..day. = 28
)
# Predict with confidence interval
print("confidence interval")
predict(model, newdata = new_data, interval = "confidence")
# Predict with prediction interval
print("prediction interval")
predict(model, newdata = new_data, interval = "prediction")
# New data point
new_data <- data.frame(
Cement = 300,
Blast.Furnace.Slag = 100,
Fly.Ash = 50,
Water = 200,
Superplasticizer = 5,
Coarse.Aggregate = 1000,
Fine.Aggregate = 800,
Age..day. = 28
)
# Predict with confidence interval
print("confidence interval")
predict(model, newdata = new_data, interval = "confidence")
# Predict with prediction interval
print("prediction interval")
predict(model, newdata = new_data, interval = "prediction")
# Total Sum of Squares (SST)
sst <- sum((data$Concrete.compressive.strength - mean(data$Concrete.compressive.strength))^2)
# Residual Sum of Squares (SSR)
ssr <- sum(residuals(model)^2)
# R-squared
r_squared <- 1 - (ssr / sst)
print(r_squared)
# RMSE and MAE
library(Metrics)
rmse(actual = data$Concrete.compressive.strength, predicted = predict(model))
mae(actual = data$Concrete.compressive.strength, predicted = predict(model))
ols_plot_resid_fit(model1)
ols_plot_resid_fit(model2)
ols_plot_resid_fit(model3)
ols_plot_resid_fit(model4)
ols_plot_resid_fit(model5)
ols_plot_resid_fit(model6)
ols_plot_resid_fit(model7)
ols_plot_resid_fit(model8)
bptest(model1, varformula = ~ Cement, data = data, studentize = FALSE)
bptest(model2, varformula = ~ Blast.Furnace.Slag, data = data, studentize = FALSE)
bptest(model3, varformula = ~ Fly.Ash, data = data, studentize = FALSE)
bptest(model4, varformula = ~ Water, data = data, studentize = FALSE)
bptest(model5, varformula = ~ Superplasticizer, data = data, studentize = FALSE)
bptest(model6, varformula = ~ Coarse.Aggregate, data = data, studentize = FALSE)
bptest(model7, varformula = ~ Fine.Aggregate, data = data, studentize = FALSE)
bptest(model8, varformula = ~ Age..day., data = data, studentize = FALSE)
summary(cars)
plot(pressure)
alpha <- 0.05
df <- 1
chi_square_critical <- qchisq(1 - alpha, df)
chi_square_critical
data$water_cement_ratio <- data$Water/data$Cement
head(data)
var <- "water_cement_ratio"
response_var <- "Concrete.compressive.strength"
f <- ggplot(data, aes_string(x = var, y = response_var)) +
geom_point(alpha = 0.6, color = "green") +
labs(title = paste("Relationship between", var, "and", response_var),
x = var, y = response_var) +
theme_minimal()
print(f)
mdel <- lm(Concrete.compressive.strength ~ ., data = data)
ols_regress(mdel) #sig. in output table represent p-value
mdel1 <- lm(Concrete.compressive.strength ~ water_cement_ratio, data = data)
ols_regress(mdel1)
ols_plot_resid_fit(mdel1)
bptest(mdel1, varformula = ~ water_cement_ratio, data = data, studentize = FALSE)
install.packages("pagedown")
knit_with_parameters("D:/Data Science/1st_Semester/Regression_model/Proposal/Compressive_Strenght_Proposal1.Rmd")
knitr::opts_chunk$set(echo = TRUE)
install.packages("tinytex")
tinytex::install_tinytex()
install.packages("tinytex")
knitr::opts_chunk$set(echo = TRUE)
install.packages("xfun")
install.packages("xfun")
update.packages(ask = FALSE)
